% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bs.R
\name{bs}
\alias{bs}
\title{Best subset selection.}
\usage{
bs(x, y, k = 1:min(nrow(x), ncol(x)), intercept = TRUE, time.limit = 100,
  nruns = 50, maxiter = 1000, tol = 1e-04, polish = TRUE,
  verbose = FALSE)
}
\arguments{
\item{x}{Matrix of predictors, of dimension (say) n x p.}

\item{y}{Vector of responses, of length (say) n.}

\item{k}{Sparsity level, i.e., number of nonzero coefficients to allow in the
subset regression model; can be a vector, in which case the best subset
selection problem is solved for every value of the sparsity level. Default
is 1:min(n,p).}

\item{intercept}{Should an intercept be included in the regression model? 
Default is TRUE.}

\item{verbose}{Should intermediate progress be printed out? Default is FALSE.}
}
\value{
A list with the following components: 
  \itemize{
  \item beta: matrix of regression coefficients, one column per sparsity
    level
  \item status: vector of status strings returned by Gurobi's MIO solver,
    one element for each sparsity level
  \item k: vector of sparsity levels
  \item x, y: the passed x and y
  \item bx, by: the means of the columns of x, and the mean of y
  \item intercept: was an intercept included?
  }
}
\description{
Compute best subset selection solutions.
}
\details{
This function solves best subset selection program:
  \deqn{\min_\beta \|Y - X \beta\|_2^2 \;\;{\rm s.t.}\;\;
    \|\beta\|_0 \leq k}
  for a response vector \eqn{Y} and predictor matrix \eqn{X}. It uses
  projected gradient descent to find an approximate solution to the
  above nonconvex program, and then calls Gurobi's MIO (mixed integer
  optimization) solver with this approximate solution as a warm start.
  See references below for the paper by Bertsimas, King, and Mazumder
  (2016), that describes this algorithm.
}
\examples{
# Simulate some simple regression data with the first 5 coefficients
# being nonzero
set.seed(3)
n = 100
p = 20
ntest = 10000
xy.obj = sim.xy(n,p,nval=0,ntest=ntest,s=5,beta.type=2,snr=1)
x = xy.obj$x
y = xy.obj$y
xtest = xy.obj$xtest
mutest = xy.obj$mutest

# Run forward stepwise regression for 8 steps
fs.obj = fs(x,y,intercept=FALSE,maxsteps=8,verbose=TRUE)
fs.beta = coef(fs.obj)
fs.supp = apply(fs.beta != 0, 2, which)

# Solve best subset selection for 8 sparsity levels
bs.obj = bs(x,y,intercept=FALSE,k=1:8,verbose=TRUE)
bs.beta = coef(bs.obj)
bs.supp = apply(bs.beta != 0, 2, which)

# Compare supports of the solutions with 5 and 8 variables
fs.supp[[5]]; bs.supp[[5]]
fs.supp[[8]]; bs.supp[[8]]

# Predict on test data and record risk
fs.pred = predict(fs.obj,newx=xtest)
bs.pred = predict(fs.obj,newx=xtest)
colMeans((fs.pred - mutest)^2)
colMeans((bs.pred - mutest)^2)
}
\author{
Ryan Tibshirani
}
\references{
This function utilizes the MIO formulation for subset selection
  as described in "Best subset selection via a modern optimization lens" by
  Dimitris Bertsimas, Angela King, and Rahul Mazumder, Annals of Statistics,
  44(2), 813-852, 2016. This R implementation is based on Matlab code written
  by Rahul Mazumder.
}

